\section{使用SVD进行PCA}
通过分析前文给出的使用统计方法进行PCA的步骤，可以发现，奇异值分解 (singular value decomposition, SVD) 也可以完成PCA。

\subsection{SVD}
对于一个$m \times n$的复矩阵$A \in \mathbb{C}^{m\times n}$，一定存在两个幺正矩阵$U \in \mathbb{C}^{m\times m}$和
$V \in \mathbb{C}^{n\times n}$，它们能够将$A$“对角化”
\begin{equation}
    U^\dagger AV = Σ = \mathrm{diag}(\sigma_1 ,\cdots,\sigma_p ) \in \mathbb{C}^{m\times n} , p = \min(m,n).
\end{equation}
其中 $\sigma_1 \ge \sigma_2 \ge ··· \ge \sigma_p \ge 0$ 称为矩阵 $A$ 的奇异值。

SVD存在较为直接、性能较高的方法，比如Golub-Kahan-Reinsch算法。
间接的、使用对称方阵本征值的方法则为
\begin{enumerate}
    \item 获得 $AA^T$ 的特征值和特征向量，用单位化的向量构成 $U$；
    \item 获得 $A^T A$ 的特征值和特征向量，用单位化的向量构成 $V$；
    \item 将 $AA^T$ 或 $A^T A$ 的特征值求平方根，构成 $\Sigma$。
\end{enumerate}
可以发现，这种方法与前文使用的统计方法十分相似。
下面，我们将使用该方法。

\subsection{SVD与PCA的具体关系}

在本题语境中，$A$即为中心化后的$m=10,000$个$n=6$维行矢量组成的矩阵。
在与前文的统计方法步骤比较后，可以发现，$V$即为各特征列向量组成的矩阵。
而原始数据在新的基上的投影即为
\begin{equation}
    AV = U\Sigma V^\dagger V = U\Sigma.
\end{equation}

注意到，$\Sigma$的下$m-n$行全为0，故SVD可等价的写为 (设$m>n$)
\begin{equation}
    A = U'\Sigma'V^\dagger,
\end{equation}
其中$\Sigma'$仅取$\Sigma$的前$n$行，$U'$仅取$U$的前$n$列。
这种SVD避免了求$10,000\times 10,000$的稠密矩阵的本征矢，较为节省时间。
SVD的具体实现请参见\verb|misc/svd.h|。

本题使用SVD解决仅需
\begin{enumerate}
    \item 对数据进行去中心化；
    \item 进行SVD；
    \item 求出$AV = U\Sigma$。
\end{enumerate}

\subsection{具体求解}
参见\verb|pca_svd.cpp|。

使用函数\verb|decentralize()|进行去中心化。
输出6维数据的各自的中心至文件\verb|output/centre.txt|。

进行简化的SVD。
输出奇异值、非平庸的$U$、$V$至文件\verb|output/singular_values.txt|、\verb|output/u_mat.txt|、\verb|output/v_mat.txt|。

计算$AV$，输出至文件\verb|output/answer_svd.txt|。
与前面得到的\verb|answer.txt|对比，除正负号外完全一致，因此不再单独给出结果。
